{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab58ea2-146b-4034-84c8-b5543f581e1a",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff9861-7ff2-44d7-9e89-09dba0b0927b",
   "metadata": {},
   "source": [
    "## Decision Tree with no depth control -> Task 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e98733-1de3-4c29-9668-008d1c82dc26",
   "metadata": {},
   "source": [
    "### Load the data and codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e06675-9179-4ac2-9a25-59fe3aca8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "init_df = pd.read_csv('data/training/adult_train_data.csv')\n",
    "test_df = pd.read_csv('data/testing/adult_test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a73eb-2927-4c5a-b21f-3ac294c6db4a",
   "metadata": {},
   "source": [
    "### Load the functions for best feature calculation based on information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b7ed13-997b-472e-beb3-8a3880677d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains functions to calculate information gain for a given dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def map_features(df):\n",
    "    \"\"\"\n",
    "    Map atrributes to their unique values.\n",
    "    \"\"\"\n",
    "    column_feature_map = {}\n",
    "    for column in df.columns:\n",
    "        column_feature_map[column] = df[column].unique().tolist()\n",
    "    return column_feature_map\n",
    "\n",
    "\n",
    "def entropy(df):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a label array.\n",
    "    \"\"\"\n",
    "    counts = df['income_more50K'].value_counts()\n",
    "    zero_count = counts.get(0, 0)\n",
    "    one_count = counts.get(1, 0)\n",
    "    total_count = zero_count + one_count\n",
    "    if zero_count == 0 or one_count == 0:\n",
    "        return 0\n",
    "    # Adding a small constant to avoid log(0)\n",
    "    entropy = -(\n",
    "        (zero_count / total_count) * np.log2(zero_count / total_count + 1e-10) +\n",
    "        (one_count / total_count) * np.log2(one_count / total_count + 1e-10)\n",
    "    )\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def information_gain(df, feature, target_column):\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a feature.\n",
    "    \"\"\"\n",
    "    initial_entropy = entropy(df)\n",
    "\n",
    "    weighted_entropy = 0\n",
    "    correct_subset = df[df[target_column] == feature]\n",
    "    incorrect_subset = df[df[target_column] != feature]\n",
    "    total_count = len(df)\n",
    "    correct_count = len(correct_subset)\n",
    "    incorrect_count = len(incorrect_subset)\n",
    "\n",
    "    correct_entropy = entropy(correct_subset)\n",
    "    incorrect_entropy = entropy(incorrect_subset)\n",
    "\n",
    "    weighted_entropy += (correct_count / total_count) * correct_entropy\n",
    "    weighted_entropy += (incorrect_count / total_count) * incorrect_entropy\n",
    "\n",
    "    return initial_entropy - weighted_entropy\n",
    "\n",
    "\n",
    "def calculate_best_feature(df):\n",
    "    \"\"\"\n",
    "    Find the best feature to split on.\n",
    "    \"\"\"\n",
    "    column_feature_map = map_features(df)\n",
    "    best_gain = -1\n",
    "    best_feature = None\n",
    "    best_feature_column = None\n",
    "\n",
    "    for column, features in column_feature_map.items():\n",
    "        if column == 'income_more50K':\n",
    "            continue\n",
    "        for feature in features:\n",
    "            gain = information_gain(df, feature, column)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_feature_column = column\n",
    "\n",
    "    return best_feature, best_feature_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c556e2c-cee7-4557-ac19-4b4f832590b4",
   "metadata": {},
   "source": [
    "### Load the TreeNode class for building decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f61f153-4e88-4c08-aea3-9fdfe6e6858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the TreeNode class, which represents a node in a decision tree.\n",
    "\"\"\"\n",
    "class TreeNode:\n",
    "    def __init__(self,\n",
    "                 feature: str = None,\n",
    "                 column: str = None,\n",
    "                 data: pd.DataFrame = None,\n",
    "                 label: str = None,\n",
    "                 index: int = None,\n",
    "                 max_depth: int = None,\n",
    "                 left=None,\n",
    "                 right=None):\n",
    "        \"\"\"\n",
    "        Initialize a tree node with the given parameters.\n",
    "        :param data: Data to train the node.\n",
    "        :param feature: Feature to split on.\n",
    "        :param column: Column name of the feature.\n",
    "        :param label: Label of the node output\n",
    "        value: Value of the node.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.feature = feature\n",
    "        self.column = column\n",
    "        self.label = label\n",
    "        self.index = index\n",
    "        self.max_depth = max_depth\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.column) + \": \" + str(self.feature)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the tree node with the given data.\n",
    "        :param data: Data to train the node.\n",
    "        \"\"\"\n",
    "        # print(self.index)\n",
    "\n",
    "        if self.data['income_more50K'].nunique() == 1:\n",
    "            self.label = self.data['income_more50K'].values[0]\n",
    "            return\n",
    "\n",
    "        if self.max_depth is not None and self.index >= self.max_depth:\n",
    "            self.label = self.data['income_more50K'].mode()[0]\n",
    "            return\n",
    "\n",
    "        self.feature, self.column = calculate_best_feature(self.data)\n",
    "        right_data = self.data[self.data[self.column] == self.feature]\n",
    "        left_data = self.data[self.data[self.column] != self.feature]\n",
    "\n",
    "        if right_data.empty or left_data.empty:\n",
    "            self.label = self.data['income_more50K'].mode()[0]\n",
    "            return\n",
    "\n",
    "        right_node = TreeNode(\n",
    "            data=right_data, index=self.index + 1, max_depth=self.max_depth)\n",
    "        left_node = TreeNode(\n",
    "            data=left_data, index=self.index + 1, max_depth=self.max_depth)\n",
    "        self.add_children(left_node, \"left\")\n",
    "        self.add_children(right_node, \"right\")\n",
    "        # Recursively train the left and right nodes\n",
    "\n",
    "        left_node.train()\n",
    "        right_node.train()\n",
    "\n",
    "    def add_children(self, tree_node, direction: str):\n",
    "        \"\"\"\n",
    "        Add children to the node.\n",
    "        \"\"\"\n",
    "\n",
    "        if direction == \"left\":\n",
    "            self.left = tree_node\n",
    "        elif direction == \"right\":\n",
    "            self.right = tree_node\n",
    "        else:\n",
    "            raise ValueError(\"Direction must be 'left' or 'right'\")\n",
    "\n",
    "    def print_tree(self, depth=0, direction=None):\n",
    "        \"\"\" Recursively print the tree structure \"\"\"\n",
    "\n",
    "        if self.index > 6:\n",
    "            return\n",
    "\n",
    "        indent = \"   \" * depth\n",
    "        if self.feature is not None:\n",
    "            if direction == \"left\":\n",
    "                print(\n",
    "                    f\"{indent}├── {self.index} No, then: check {self.column} to be {self.feature}\")\n",
    "            elif direction == \"right\":\n",
    "                print(\n",
    "                    f\"{indent}├── {self.index} Yes, then: check {self.column} to be {self.feature}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"{indent}├── {self.index} Root: check {self.column} to be {self.feature}\")\n",
    "        else:\n",
    "            if direction == \"left\":\n",
    "                print(f\"{indent}├── {self.index} No, then: output {self.label}\")\n",
    "            elif direction == \"right\":\n",
    "                print(f\"{indent}├── {self.index} Yes, then: output {self.label}\")\n",
    "\n",
    "        if self.right is not None:\n",
    "            self.right.print_tree(depth=depth + 1, direction=\"right\")\n",
    "        if self.left is not None:\n",
    "            self.left.print_tree(depth=depth + 1, direction=\"left\")\n",
    "\n",
    "    def predict(self, row):\n",
    "        \"\"\"\n",
    "        Predict the label for the given row.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.label is not None:\n",
    "            return int(self.label)\n",
    "\n",
    "        if row[self.column] == self.feature:\n",
    "            return self.right.predict(row)\n",
    "        else:\n",
    "            return self.left.predict(row)\n",
    "\n",
    "    def predict_all(self, df):\n",
    "        \"\"\"\n",
    "        Predict the labels for all rows in the given DataFrame.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _, row in df.iterrows():\n",
    "            prediction = self.predict(row)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the decision tree on the given DataFrame.\n",
    "        \"\"\"\n",
    "        predictions = self.predict_all(df)\n",
    "        correct_predictions = sum(predictions == df['income_more50K'])\n",
    "        accuracy = correct_predictions / len(df)\n",
    "        return accuracy\n",
    "\n",
    "    def recall(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the recall of the decision tree on the given DataFrame.\n",
    "        \"\"\"\n",
    "        predictions = self.predict_all(df)\n",
    "        true_positives = sum(\n",
    "            (predictions == df['income_more50K']) & (df['income_more50K'] == 1))\n",
    "        false_negatives = sum(\n",
    "            (predictions != df['income_more50K']) & (df['income_more50K'] == 1))\n",
    "\n",
    "        if true_positives + false_negatives == 0:\n",
    "            return 0\n",
    "\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        return recall\n",
    "\n",
    "    def precision(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the precision of the decision tree on the given DataFrame.\n",
    "        \"\"\"\n",
    "        predictions = self.predict_all(df)\n",
    "        true_positives = sum(\n",
    "            (predictions == df['income_more50K']) & (df['income_more50K'] == 1))\n",
    "        false_positives = sum(\n",
    "            (predictions != df['income_more50K']) & (df['income_more50K'] == 0))\n",
    "\n",
    "        if true_positives + false_positives == 0:\n",
    "            return 0\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        return precision\n",
    "\n",
    "    def f1_score(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the F1 score of the decision tree on the given DataFrame.\n",
    "        \"\"\"\n",
    "        recall = self.recall(df)\n",
    "        precision = self.precision(df)\n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    def confusion_matrix(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the confusion matrix of the decision tree on the given DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = self.predict_all(df)\n",
    "        true_positives = sum(\n",
    "            (predictions == df['income_more50K']) & (df['income_more50K'] == 1))\n",
    "        false_positives = sum(\n",
    "            (predictions != df['income_more50K']) & (df['income_more50K'] == 0))\n",
    "        false_negatives = sum(\n",
    "            (predictions != df['income_more50K']) & (df['income_more50K'] == 1))\n",
    "        true_negatives = sum(\n",
    "            (predictions == df['income_more50K']) & (df['income_more50K'] == 0))\n",
    "\n",
    "        matrix = pd.DataFrame(\n",
    "            [[true_positives, false_negatives], [false_positives, true_negatives]],\n",
    "            index=[\"Actual Positive\", \"Actual Negative\"],\n",
    "            columns=[\"Predicted Positive\", \"Predicted Negative\"]\n",
    "        )\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def print_statistics(self, df):\n",
    "        \"\"\"\n",
    "        Print the statistics of the decision tree on the given DataFrame.\n",
    "        \"\"\"\n",
    "        print(\"\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\"\")\n",
    "        print(f\"Decision Tree Statistics for max depth {self.max_depth}:\")\n",
    "        print(\"\")\n",
    "        print(f\"Training Accuracy: {self.accuracy(self.data) * 100:.2f}%\")\n",
    "        print(f\"Testing Accuracy: {self.accuracy(df) * 100:.2f}%\")\n",
    "        print(f\"Recall: {self.recall(df) * 100:.2f}%\")\n",
    "        print(f\"Precision: {self.precision(df) * 100:.2f}%\")\n",
    "        print(f\"F1 Score: {self.f1_score(df) * 100:.2f}%\")\n",
    "\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(self.confusion_matrix(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20196047-e8f9-4b5c-9437-0bfa7bced834",
   "metadata": {},
   "source": [
    "### Train the model and print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00689c97-0cab-4b5c-bda3-472671c93c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── 0 Root: check marital_status to be Married-civ-spouse\n",
      "   ├── 1 Yes, then: check education_num to be >=10\n",
      "      ├── 2 Yes, then: check education to be Some-college\n",
      "         ├── 3 Yes, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: check hours_per_week to be <35\n",
      "               ├── 5 Yes, then: check fnlwgt to be 200K-300K\n",
      "                  ├── 6 Yes, then: check occupation to be Adm-clerical\n",
      "                  ├── 6 No, then: output 0\n",
      "               ├── 5 No, then: check occupation to be Other-service\n",
      "                  ├── 6 Yes, then: check fnlwgt to be 200K-300K\n",
      "                  ├── 6 No, then: check occupation to be Machine-op-inspct\n",
      "            ├── 4 No, then: check occupation to be Other-service\n",
      "               ├── 5 Yes, then: check age to be 40-49\n",
      "                  ├── 6 Yes, then: check relationship to be Own-child\n",
      "                  ├── 6 No, then: check age to be >=60\n",
      "               ├── 5 No, then: check occupation to be Farming-fishing\n",
      "                  ├── 6 Yes, then: check age to be >=60\n",
      "                  ├── 6 No, then: check age to be 30-39\n",
      "         ├── 3 No, then: check education to be Assoc-voc\n",
      "            ├── 4 Yes, then: check age to be 20-29\n",
      "               ├── 5 Yes, then: check workclass to be Self-emp-not-inc\n",
      "                  ├── 6 Yes, then: output 0\n",
      "                  ├── 6 No, then: check workclass to be Private\n",
      "               ├── 5 No, then: check occupation to be Exec-managerial\n",
      "                  ├── 6 Yes, then: check hours_per_week to be 35-40\n",
      "                  ├── 6 No, then: check occupation to be Prof-specialty\n",
      "            ├── 4 No, then: check age to be 20-29\n",
      "               ├── 5 Yes, then: check hours_per_week to be <35\n",
      "                  ├── 6 Yes, then: check relationship to be Wife\n",
      "                  ├── 6 No, then: check education to be Assoc-acdm\n",
      "               ├── 5 No, then: check occupation to be Exec-managerial\n",
      "                  ├── 6 Yes, then: check hours_per_week to be >40\n",
      "                  ├── 6 No, then: check occupation to be Prof-specialty\n",
      "      ├── 2 No, then: check education_num to be <8\n",
      "         ├── 3 Yes, then: check hours_per_week to be >40\n",
      "            ├── 4 Yes, then: check occupation to be Exec-managerial\n",
      "               ├── 5 Yes, then: check education to be 1st-4th\n",
      "                  ├── 6 Yes, then: output 1\n",
      "                  ├── 6 No, then: check education to be 5th-6th\n",
      "               ├── 5 No, then: check occupation to be Other-service\n",
      "                  ├── 6 Yes, then: output 0\n",
      "                  ├── 6 No, then: check relationship to be Not-in-family\n",
      "            ├── 4 No, then: check education to be 10th\n",
      "               ├── 5 Yes, then: check age to be 20-29\n",
      "                  ├── 6 Yes, then: check workclass to be Self-emp-not-inc\n",
      "                  ├── 6 No, then: check occupation to be Adm-clerical\n",
      "               ├── 5 No, then: check workclass to be Self-emp-inc\n",
      "                  ├── 6 Yes, then: check fnlwgt to be <100K\n",
      "                  ├── 6 No, then: check education to be 11th\n",
      "         ├── 3 No, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: check occupation to be Prof-specialty\n",
      "               ├── 5 Yes, then: check fnlwgt to be 200K-300K\n",
      "                  ├── 6 Yes, then: output 1\n",
      "                  ├── 6 No, then: check native_country to be United-States\n",
      "               ├── 5 No, then: check occupation to be Exec-managerial\n",
      "                  ├── 6 Yes, then: check workclass to be Self-emp-not-inc\n",
      "                  ├── 6 No, then: check relationship to be Wife\n",
      "            ├── 4 No, then: check hours_per_week to be <35\n",
      "               ├── 5 Yes, then: check relationship to be Wife\n",
      "                  ├── 6 Yes, then: check fnlwgt to be 200K-300K\n",
      "                  ├── 6 No, then: check workclass to be Self-emp-inc\n",
      "               ├── 5 No, then: check occupation to be Exec-managerial\n",
      "                  ├── 6 Yes, then: check workclass to be Self-emp-not-inc\n",
      "                  ├── 6 No, then: check occupation to be Other-service\n",
      "   ├── 1 No, then: check hours_per_week to be >40\n",
      "      ├── 2 Yes, then: check education_num to be >=10\n",
      "         ├── 3 Yes, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: check education to be Some-college\n",
      "               ├── 5 Yes, then: check workclass to be Self-emp-not-inc\n",
      "                  ├── 6 Yes, then: check occupation to be Sales\n",
      "                  ├── 6 No, then: check relationship to be Not-in-family\n",
      "               ├── 5 No, then: check marital_status to be Married-AF-spouse\n",
      "                  ├── 6 Yes, then: output 1\n",
      "                  ├── 6 No, then: check sex to be Male\n",
      "            ├── 4 No, then: check education to be Some-college\n",
      "               ├── 5 Yes, then: check age to be 30-39\n",
      "                  ├── 6 Yes, then: check relationship to be Not-in-family\n",
      "                  ├── 6 No, then: check sex to be Male\n",
      "               ├── 5 No, then: check occupation to be Exec-managerial\n",
      "                  ├── 6 Yes, then: check education to be Masters\n",
      "                  ├── 6 No, then: check education to be Prof-school\n",
      "         ├── 3 No, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: check workclass to be Private\n",
      "               ├── 5 Yes, then: check native_country to be Portugal\n",
      "                  ├── 6 Yes, then: check fnlwgt to be <100K\n",
      "                  ├── 6 No, then: check fnlwgt to be 100K-200K\n",
      "               ├── 5 No, then: check relationship to be Own-child\n",
      "                  ├── 6 Yes, then: output 0\n",
      "                  ├── 6 No, then: check occupation to be Exec-managerial\n",
      "            ├── 4 No, then: check occupation to be Machine-op-inspct\n",
      "               ├── 5 Yes, then: output 0\n",
      "               ├── 5 No, then: check sex to be Male\n",
      "                  ├── 6 Yes, then: check occupation to be Handlers-cleaners\n",
      "                  ├── 6 No, then: check marital_status to be Married-AF-spouse\n",
      "      ├── 2 No, then: check occupation to be Prof-specialty\n",
      "         ├── 3 Yes, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: check hours_per_week to be 35-40\n",
      "               ├── 5 Yes, then: check workclass to be Local-gov\n",
      "                  ├── 6 Yes, then: output 0\n",
      "                  ├── 6 No, then: check fnlwgt to be <100K\n",
      "               ├── 5 No, then: check workclass to be Self-emp-not-inc\n",
      "                  ├── 6 Yes, then: check fnlwgt to be <100K\n",
      "                  ├── 6 No, then: output 0\n",
      "            ├── 4 No, then: check education to be Prof-school\n",
      "               ├── 5 Yes, then: check sex to be Male\n",
      "                  ├── 6 Yes, then: check age to be 40-49\n",
      "                  ├── 6 No, then: check native_country to be Germany\n",
      "               ├── 5 No, then: check education to be Doctorate\n",
      "                  ├── 6 Yes, then: check workclass to be Local-gov\n",
      "                  ├── 6 No, then: check age to be <20\n",
      "         ├── 3 No, then: check occupation to be Exec-managerial\n",
      "            ├── 4 Yes, then: check age to be 20-29\n",
      "               ├── 5 Yes, then: check relationship to be Own-child\n",
      "                  ├── 6 Yes, then: output 0\n",
      "                  ├── 6 No, then: check workclass to be Self-emp-not-inc\n",
      "               ├── 5 No, then: check sex to be Male\n",
      "                  ├── 6 Yes, then: check relationship to be Own-child\n",
      "                  ├── 6 No, then: check education to be Masters\n",
      "            ├── 4 No, then: check relationship to be Own-child\n",
      "               ├── 5 Yes, then: check age to be <20\n",
      "                  ├── 6 Yes, then: output 0\n",
      "                  ├── 6 No, then: check age to be 20-29\n",
      "               ├── 5 No, then: check education to be Bachelors\n",
      "                  ├── 6 Yes, then: check age to be 20-29\n",
      "                  ├── 6 No, then: check occupation to be Other-service\n"
     ]
    }
   ],
   "source": [
    "decision_tree_no_max_depth = TreeNode(data=init_df, index=0)\n",
    "decision_tree_no_max_depth.train()\n",
    "decision_tree_no_max_depth.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590722d-2d18-4889-8be5-9901aace77dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h3>Task 1A</h3>\n",
    "<h4>\n",
    "    With no depth control, the training phase only stops if all labels at the node are the same and output the label or all data at the node have same attributes such that we cannot split using the criterion anymore.\n",
    "</h4>\n",
    "<h4>\n",
    "    As the decision is too big with no depth control, I have only printed its first 6 levels. The condition at each check statement would be the feature that gives the best information gain when splitting at the node.\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a0ecf-fd71-4b4a-8165-acb63797c1f6",
   "metadata": {},
   "source": [
    "## Decision Tree with depth control -> Task 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441cda3-e2ed-4e83-9373-84fcc4a71fe4",
   "metadata": {},
   "source": [
    "### Tree with max depth of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9db2bf5-45ec-410c-8a26-5f5485201674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── 0 Root: check marital_status to be Married-civ-spouse\n",
      "   ├── 1 Yes, then: check education_num to be >=10\n",
      "      ├── 2 Yes, then: output 1\n",
      "      ├── 2 No, then: output 0\n",
      "   ├── 1 No, then: check hours_per_week to be >40\n",
      "      ├── 2 Yes, then: output 0\n",
      "      ├── 2 No, then: output 0\n"
     ]
    }
   ],
   "source": [
    "decision_tree_max_depth_2 = TreeNode(data=init_df, index=0, max_depth=2)\n",
    "decision_tree_max_depth_2.train()\n",
    "decision_tree_max_depth_2.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d9ab1-8fd6-4f26-8122-3d93971eb65d",
   "metadata": {},
   "source": [
    "### Tree with max depth of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fe2dda-101b-4415-82d1-1f529b9c1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── 0 Root: check marital_status to be Married-civ-spouse\n",
      "   ├── 1 Yes, then: check education_num to be >=10\n",
      "      ├── 2 Yes, then: check education to be Some-college\n",
      "         ├── 3 Yes, then: output 0\n",
      "         ├── 3 No, then: output 1\n",
      "      ├── 2 No, then: check education_num to be <8\n",
      "         ├── 3 Yes, then: output 0\n",
      "         ├── 3 No, then: output 0\n",
      "   ├── 1 No, then: check hours_per_week to be >40\n",
      "      ├── 2 Yes, then: check education_num to be >=10\n",
      "         ├── 3 Yes, then: output 0\n",
      "         ├── 3 No, then: output 0\n",
      "      ├── 2 No, then: check occupation to be Prof-specialty\n",
      "         ├── 3 Yes, then: output 0\n",
      "         ├── 3 No, then: output 0\n"
     ]
    }
   ],
   "source": [
    "decision_tree_max_depth_3 = TreeNode(data=init_df, index=0, max_depth=3)\n",
    "decision_tree_max_depth_3.train()\n",
    "decision_tree_max_depth_3.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01909cad-a4b3-41d3-80b8-c593e01a4fb6",
   "metadata": {},
   "source": [
    "### Tree with max depth of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7acd104-2f6f-465a-a797-5a26dd4450ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── 0 Root: check marital_status to be Married-civ-spouse\n",
      "   ├── 1 Yes, then: check education_num to be >=10\n",
      "      ├── 2 Yes, then: check education to be Some-college\n",
      "         ├── 3 Yes, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 0\n",
      "         ├── 3 No, then: check education to be Assoc-voc\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 1\n",
      "      ├── 2 No, then: check education_num to be <8\n",
      "         ├── 3 Yes, then: check hours_per_week to be >40\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 0\n",
      "         ├── 3 No, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 0\n",
      "   ├── 1 No, then: check hours_per_week to be >40\n",
      "      ├── 2 Yes, then: check education_num to be >=10\n",
      "         ├── 3 Yes, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 0\n",
      "         ├── 3 No, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 0\n",
      "      ├── 2 No, then: check occupation to be Prof-specialty\n",
      "         ├── 3 Yes, then: check age to be 20-29\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 0\n",
      "         ├── 3 No, then: check occupation to be Exec-managerial\n",
      "            ├── 4 Yes, then: output 0\n",
      "            ├── 4 No, then: output 0\n"
     ]
    }
   ],
   "source": [
    "decision_tree_max_depth_4 = TreeNode(data=init_df, index=0, max_depth=4)\n",
    "decision_tree_max_depth_4.train()\n",
    "decision_tree_max_depth_4.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed03a5-9eab-43d6-8f02-741f0808eb16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h3>Task 1B</h3>\n",
    "<h4>\n",
    "    Now with depth control, we just added another stopping criterion for the training phase. When the tree reaches the max depth, the node will immediately output the mode of the labels at the node, even if we can still split on the data. It is also seen that both directions of the node could output the same label as the prediction, and it becomes unnesscessary to check the last feature. This is because the labels have not been separated well, and 0 label is dominating the training dataset. Therefore, if both leaves output the mode, they would both predict 0.\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4f4b0-24f6-4d13-8328-33af558cd6e9",
   "metadata": {},
   "source": [
    "## Test Decision Tree with no depth control -> Task 1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7bd50b-846e-4961-876b-613538702dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Decision Tree Statistics for max depth 3:\n",
      "\n",
      "Training Accuracy: 81.29%\n",
      "Testing Accuracy: 81.46%\n",
      "Recall: 47.62%\n",
      "Precision: 67.35%\n",
      "F1 Score: 55.79%\n",
      "Confusion Matrix:\n",
      "                 Predicted Positive  Predicted Negative\n",
      "Actual Positive                1762                1938\n",
      "Actual Negative                 854               10506\n"
     ]
    }
   ],
   "source": [
    "decision_tree_max_depth_3.print_statistics(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8bf31-9393-41b3-86a8-86df743120d3",
   "metadata": {},
   "source": [
    "<h3>Task 1C</h3>\n",
    "<h4>\n",
    "    In general, this decision tree fits the training data reasonably well, with a training accuracy of 82.29%. It also has a reasonably well performance on new data with a testing accuracy of 81.46%, which gives a very small variance. This indicates a very good generalisation of the model with a max depth of 3.\n",
    "</h4>\n",
    "\n",
    "<h4>\n",
    "    More specifically, its recall is only 47.62%, while its precision is 67.35%. A low recall combined with a high precision suggests that the model is overly conservative in classifying positives, and it only predicts them when it is very confident. This is likely to be caused by the imbalance of the labels in the training dataset, where the negative examples dominate. When we output the mode at the node, the labels have not split very well at a depth of 3, and the dominating negative examples will cause the model to ignore the positive examples. This can be seen from the tree printed in 1B, that only one leaf outputs 1 while all others output 0.\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4eaf2f-b28c-4221-8bcf-b2cbdc51c500",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c5bc2-db80-4020-91a3-a7ea2b862816",
   "metadata": {},
   "source": [
    "### Task 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a60785-84a5-4942-b7fe-838e3c168908",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h4>\n",
    "    If we change the splitting criterion to accuracy instead of information gain, the tree will prioritize splitting on the feature that maximizes prediction accuracy at each node. This could result in faster label separation at lower levels of the tree, potentially improving accuracy in the early stages. However, this approach ignores class distribution and the relationship between the label and other attributes, increasing the risk of overfitting. Since the model is not optimizing for information gain, it may focus on short-term accuracy rather than truly understanding how each feature contributes to the outcome, leading to a less generalizable model.\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1de4c-72ec-4f00-826e-e410dc055c09",
   "metadata": {},
   "source": [
    "### Task 2B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305cc69-cf06-486d-b6aa-3daf91012bbc",
   "metadata": {},
   "source": [
    "#### Print out the stats of each tree for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f657f89a-a0a7-410e-a11d-16eb59d3111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Decision Tree Statistics for max depth 2:\n",
      "\n",
      "Training Accuracy: 80.22%\n",
      "Testing Accuracy: 80.66%\n",
      "Recall: 63.41%\n",
      "Precision: 60.09%\n",
      "F1 Score: 61.70%\n",
      "Confusion Matrix:\n",
      "                 Predicted Positive  Predicted Negative\n",
      "Actual Positive                2346                1354\n",
      "Actual Negative                1558                9802\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Decision Tree Statistics for max depth 3:\n",
      "\n",
      "Training Accuracy: 81.29%\n",
      "Testing Accuracy: 81.46%\n",
      "Recall: 47.62%\n",
      "Precision: 67.35%\n",
      "F1 Score: 55.79%\n",
      "Confusion Matrix:\n",
      "                 Predicted Positive  Predicted Negative\n",
      "Actual Positive                1762                1938\n",
      "Actual Negative                 854               10506\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Decision Tree Statistics for max depth 4:\n",
      "\n",
      "Training Accuracy: 81.49%\n",
      "Testing Accuracy: 81.70%\n",
      "Recall: 43.89%\n",
      "Precision: 70.49%\n",
      "F1 Score: 54.10%\n",
      "Confusion Matrix:\n",
      "                 Predicted Positive  Predicted Negative\n",
      "Actual Positive                1624                2076\n",
      "Actual Negative                 680               10680\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Decision Tree Statistics for max depth None:\n",
      "\n",
      "Training Accuracy: 92.57%\n",
      "Testing Accuracy: 79.53%\n",
      "Recall: 53.92%\n",
      "Precision: 59.15%\n",
      "F1 Score: 56.41%\n",
      "Confusion Matrix:\n",
      "                 Predicted Positive  Predicted Negative\n",
      "Actual Positive                1995                1705\n",
      "Actual Negative                1378                9982\n"
     ]
    }
   ],
   "source": [
    "decision_tree_max_depth_2.print_statistics(test_df)\n",
    "decision_tree_max_depth_3.print_statistics(test_df)\n",
    "decision_tree_max_depth_4.print_statistics(test_df)\n",
    "decision_tree_no_max_depth.print_statistics(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33d86e-0794-46fa-90f6-1345a23dc0b2",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    The tree with no depth control exhibits high variance, as indicated by the large gap between training and testing accuracy. This suggests that the model is overfitting the training data due to its complexity, capturing noise rather than generalizable patterns.\n",
    "</h4>\n",
    "\n",
    "<h4>\n",
    "    Trees with a max depth of 3 and 4 show good generalization, as the difference between training and testing accuracy is minimal. However, their precision and recall are imbalanced, indicating that they are too strict in classifying positive examples. This could suggest underfitting, as the model struggles to effectively separate positive from negative examples. Given the imbalance in the training data (more negative examples), the model tends to classify more instances as negative.\n",
    "</h4>\n",
    "\n",
    "<h4>\n",
    "    The tree with a maximum depth of 2 performs reasonably well overall. However, when compared to the deeper trees, testing accuracy tends to increase as complexity grows. This suggests that the model may benefit from greater depth, implying that it is also underfitting at this level.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613999c-c61d-4dcc-bfde-fe725be5811e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
